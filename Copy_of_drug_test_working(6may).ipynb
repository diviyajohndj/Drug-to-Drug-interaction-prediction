{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of drug_test_working(6may).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/diviyajohndj/Drug-to-Drug-interaction-prediction/blob/main/Copy_of_drug_test_working(6may).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkJoO7k7ihlN",
        "outputId": "69f7f004-d450-4162-e3d8-6a7b392f8312"
      },
      "source": [
        "repo_url= \"https://github.com/diviyajohndj/Drug-to-Drug-interaction-prediction\"\n",
        "import os\n",
        "\n",
        "%cd /content\n",
        "\n",
        "repo_dir_path = os.path.abspath(os.path.join('.', os.path.basename(repo_url)))\n",
        "\n",
        "!git clone {repo_url}\n",
        "%cd {repo_dir_path}\n",
        "!git pull"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "fatal: destination path 'Drug-to-Drug-interaction-prediction' already exists and is not an empty directory.\n",
            "/content/Drug-to-Drug-interaction-prediction\n",
            "Already up to date.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RchJdQuzk_6i"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "#from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection  import GridSearchCV\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "import gensim\n",
        "import spacy\n",
        "from spacy.symbols import nsubj, VERB\n",
        "from spacy.symbols import det\n",
        "\n",
        "nlp = spacy.load('en')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOI0LRlflM3o"
      },
      "source": [
        "train_data_f = \"train_binary_offsets.csv\"\n",
        "test_data_f = \"test_binary_offsets.csv\"\n",
        "#input drug\n",
        "idata_f=\"input.csv\"\n",
        "\n",
        "\n",
        "df_train = pd.read_csv(train_data_f, low_memory=False)\n",
        "df_test = pd.read_csv(test_data_f, low_memory=False)\n",
        "#input\n",
        "idata=pd.read_csv(idata_f)\n",
        "\n",
        "\n",
        "df_train = df_train[df_train['drug1'] != df_train['drug2']]\n",
        "df_test = df_test[df_test['drug1'] != df_test['drug2']]\n",
        "#input\n",
        "idata=idata[idata['drug1']!=idata['drug2']]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "zvKi1ERGlSuL",
        "outputId": "6f61a4f1-a44c-4527-edd6-b0b1feaee288"
      },
      "source": [
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentenceid</th>\n",
              "      <th>sentence_text</th>\n",
              "      <th>drug_pair</th>\n",
              "      <th>ddi_label</th>\n",
              "      <th>drug1</th>\n",
              "      <th>drug2</th>\n",
              "      <th>drug1offset</th>\n",
              "      <th>drug2offset</th>\n",
              "      <th>drugsinsent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>DDI-DrugBank.d519.s3</td>\n",
              "      <td>Laboratory Tests Response to Plenaxis should b...</td>\n",
              "      <td>DDI-DrugBank.d519.s3.p0</td>\n",
              "      <td>0</td>\n",
              "      <td>Plenaxis</td>\n",
              "      <td>testosterone</td>\n",
              "      <td>29-36</td>\n",
              "      <td>83-94</td>\n",
              "      <td>Plenaxis,testosterone,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>DDI-DrugBank.d297.s1</td>\n",
              "      <td>Population pharmacokinetic analyses revealed t...</td>\n",
              "      <td>DDI-DrugBank.d297.s1.p0</td>\n",
              "      <td>0</td>\n",
              "      <td>MTX</td>\n",
              "      <td>NSAIDs</td>\n",
              "      <td>50-52</td>\n",
              "      <td>55-60</td>\n",
              "      <td>MTX,NSAIDs,corticosteroids,TNF blocking agents...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>DDI-DrugBank.d297.s1</td>\n",
              "      <td>Population pharmacokinetic analyses revealed t...</td>\n",
              "      <td>DDI-DrugBank.d297.s1.p1</td>\n",
              "      <td>0</td>\n",
              "      <td>MTX</td>\n",
              "      <td>corticosteroids</td>\n",
              "      <td>50-52</td>\n",
              "      <td>63-77</td>\n",
              "      <td>MTX,NSAIDs,corticosteroids,TNF blocking agents...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>DDI-DrugBank.d297.s1</td>\n",
              "      <td>Population pharmacokinetic analyses revealed t...</td>\n",
              "      <td>DDI-DrugBank.d297.s1.p2</td>\n",
              "      <td>0</td>\n",
              "      <td>MTX</td>\n",
              "      <td>TNF blocking agents</td>\n",
              "      <td>50-52</td>\n",
              "      <td>84-102</td>\n",
              "      <td>MTX,NSAIDs,corticosteroids,TNF blocking agents...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>DDI-DrugBank.d297.s1</td>\n",
              "      <td>Population pharmacokinetic analyses revealed t...</td>\n",
              "      <td>DDI-DrugBank.d297.s1.p3</td>\n",
              "      <td>0</td>\n",
              "      <td>MTX</td>\n",
              "      <td>abatacept</td>\n",
              "      <td>50-52</td>\n",
              "      <td>122-130</td>\n",
              "      <td>MTX,NSAIDs,corticosteroids,TNF blocking agents...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             sentenceid  ...                                        drugsinsent\n",
              "0  DDI-DrugBank.d519.s3  ...                             Plenaxis,testosterone,\n",
              "1  DDI-DrugBank.d297.s1  ...  MTX,NSAIDs,corticosteroids,TNF blocking agents...\n",
              "2  DDI-DrugBank.d297.s1  ...  MTX,NSAIDs,corticosteroids,TNF blocking agents...\n",
              "3  DDI-DrugBank.d297.s1  ...  MTX,NSAIDs,corticosteroids,TNF blocking agents...\n",
              "4  DDI-DrugBank.d297.s1  ...  MTX,NSAIDs,corticosteroids,TNF blocking agents...\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 474
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZDZi2IOlXom",
        "outputId": "5e7be86c-37c6-440c-a787-9885abcae06b"
      },
      "source": [
        "print(\"Length of train dataset: \", df_train.shape)\n",
        "print(\"Number of drug paris with no affect on each other: \", df_train.loc[df_train['ddi_label'] == 0].shape)\n",
        "print(\"Number of drug pairs that do affect each other: \", df_train.loc[df_train['ddi_label'] == 1].shape)\n",
        "\n",
        "print(\"\\nLength of test dataset: \", df_test.shape)\n",
        "print(\"Number of drug paris with no affect on each other: \", df_test.loc[df_test['ddi_label'] == 0].shape)\n",
        "print(\"Number of drug pairs that do affect each other: \", df_test.loc[df_test['ddi_label'] == 1].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of train dataset:  (223, 9)\n",
            "Number of drug paris with no affect on each other:  (217, 9)\n",
            "Number of drug pairs that do affect each other:  (6, 9)\n",
            "\n",
            "Length of test dataset:  (209, 9)\n",
            "Number of drug paris with no affect on each other:  (183, 9)\n",
            "Number of drug pairs that do affect each other:  (26, 9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLOIjLI_mENL"
      },
      "source": [
        "def create_train_dev_test(df):\n",
        "    random_index = np.random.permutation(df.index)\n",
        "    df_shuffled = df.loc[random_index]\n",
        "    df_shuffled.reset_index(drop=True, inplace=True)\n",
        "    rows, columns =  df_shuffled.shape\n",
        "    train_size = round(rows*.2)\n",
        "    dev_size   = round(rows*.4)\n",
        "    df_train = df_shuffled.loc[:train_size]\n",
        "    df_dev = df_shuffled.loc[train_size:dev_size+train_size].reset_index(drop=True)\n",
        "    return df_train, df_dev\n",
        "\n",
        "# df_train.loc[df_train['ddi_label'] == 0].head()\n",
        "df_train_no_interact, df_dev = create_train_dev_test(df_train.loc[df_train['ddi_label'] == 0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPkj-sGemWgA",
        "outputId": "03b39127-0d7f-424e-f138-5f56dad0819c"
      },
      "source": [
        "frames = [df_train_no_interact, df_train.loc[df_train['ddi_label'] == 1]]\n",
        "df_train = pd.concat(frames)\n",
        "df_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50, 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 477
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gncTg9Szm_ku",
        "outputId": "8a68f6f2-49f4-4052-ed8d-9ce2ff50046b"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 478
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBW3yKOmmYk2",
        "outputId": "dc9cc5a1-a01d-43ed-a6c7-2ab3e36abb16"
      },
      "source": [
        "triggers = []\n",
        "with open(\"triggers.txt\") as f:\n",
        "    for line in f:\n",
        "        print(line)\n",
        "        if line!=\"\":\n",
        "          triggers.append(line)\n",
        "trigger_words = set(triggers)\n",
        "\n",
        "negative_words = ['No', 'not', 'neither', 'without', 'lack', 'fail', \n",
        "                  'unable', 'abrogate', 'absence', 'prevent', \n",
        "                  'unlikely', 'unchanged', 'rarely']\n",
        "\n",
        "negative_words = set(negative_words)\n",
        "\n",
        "def tokenize(sentence_df):\n",
        "    sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "    sentences = [sentence for sentence in sentence_df]\n",
        "    words_in_sents = [nltk.word_tokenize(sent) for sent in sentences]\n",
        "    return words_in_sents\n",
        "words_in_sents = tokenize(df_train['sentence_text'])\n",
        "ddi_embedding_model = gensim.models.Word2Vec(words_in_sents)\n",
        "ddi_embedding_model.save('ddi.embedding')\n",
        "new_model = gensim.models.Word2Vec.load('ddi.embedding')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accident\n",
            "\n",
            "ACLS\n",
            "\n",
            "Acute renal failure\n",
            "\n",
            "Agitation\n",
            "\n",
            "AKI\n",
            "\n",
            "Allergic\n",
            "\n",
            "Allergy\n",
            "\n",
            "AMS\n",
            "\n",
            "ARF\n",
            "\n",
            "Arrest\n",
            "\n",
            "Bleed\n",
            "\n",
            "Blood\n",
            "\n",
            "Bradycardia\n",
            "\n",
            "Bradicardic\n",
            "\n",
            "Blocking\n",
            "\n",
            "C diff\n",
            "\n",
            "Changes\n",
            "\n",
            "Code\n",
            "\n",
            "Coma\n",
            "\n",
            "Complication\n",
            "\n",
            "Concern\n",
            "\n",
            "Condition C\n",
            "\n",
            "Contrast Nephropathy\n",
            "\n",
            "Control\n",
            "\n",
            "d/c\n",
            "\n",
            "D50\n",
            "\n",
            "Dcâ€™d\n",
            "\n",
            "Decrease\n",
            "\n",
            "Deficiency\n",
            "\n",
            "Delirium\n",
            "\n",
            "Desaturation\n",
            "\n",
            "Diarrhea\n",
            "\n",
            "Discontinue\n",
            "\n",
            "DKA\n",
            "\n",
            "Drop\n",
            "\n",
            "Drowsy\n",
            "\n",
            "Drug fever\n",
            "\n",
            "Drug-induced\n",
            "\n",
            "Due to\n",
            "\n",
            "DVT\n",
            "\n",
            "Effect of\n",
            "\n",
            "Elevated\n",
            "\n",
            "Epistaxis\n",
            "\n",
            "Error\n",
            "\n",
            "ETOH abuse\n",
            "\n",
            "FFP\n",
            "\n",
            "Fluid resuscitation\n",
            "\n",
            "Glucagon\n",
            "\n",
            "Hallucinations\n",
            "\n",
            "Held\n",
            "\n",
            "Hematoma\n",
            "\n",
            "Hemoptysis\n",
            "\n",
            "High INR\n",
            "\n",
            "HIT\n",
            "\n",
            "Hives\n",
            "\n",
            "Hold\n",
            "\n",
            "Hyperglycemia\n",
            "\n",
            "Hypernatremia\n",
            "\n",
            "Hypoglycemia\n",
            "\n",
            "Hypotension\n",
            "\n",
            "Hypotensive\n",
            "\n",
            "Hypoxemia\n",
            "\n",
            "Hypoxia\n",
            "\n",
            "Hypoxic\n",
            "\n",
            "Iatrogenic\n",
            "\n",
            "Incorrect\n",
            "\n",
            "Ingestion\n",
            "\n",
            "Injury\n",
            "\n",
            "Interaction\n",
            "\n",
            "Intubate\n",
            "\n",
            "Intubation\n",
            "\n",
            "Lethargic\n",
            "\n",
            "Low\n",
            "\n",
            "Marijuana\n",
            "\n",
            "Melena\n",
            "\n",
            "Mental status\n",
            "\n",
            "Metabolic acidosis\n",
            "\n",
            "NAC\n",
            "\n",
            "Narcan\n",
            "\n",
            "Non-compliance\n",
            "\n",
            "Noncompliance\n",
            "\n",
            "OD\n",
            "\n",
            "Opiates\n",
            "\n",
            "Overdose\n",
            "\n",
            "Overload\n",
            "\n",
            "Oversedation\n",
            "\n",
            "Pancytopenia\n",
            "\n",
            "Polysubstance\n",
            "\n",
            "Positive\n",
            "\n",
            "PRBC\n",
            "\n",
            "Prolonged QTc\n",
            "\n",
            "Pulmonary Fibrosis\n",
            "\n",
            "Rash\n",
            "\n",
            "Rebound\n",
            "\n",
            "Related\n",
            "\n",
            "Renal Disease\n",
            "\n",
            "Renal Failure\n",
            "\n",
            "Respiratory Distress\n",
            "\n",
            "Respiratory Failure\n",
            "\n",
            "Rhadomyolysis\n",
            "\n",
            "Rise\n",
            "\n",
            "Rising\n",
            "\n",
            "SAH\n",
            "\n",
            "Secondary to\n",
            "\n",
            "Sedated\n",
            "\n",
            "Sedating Medications\n",
            "\n",
            "Seizure\n",
            "\n",
            "Serotonin Syndrome\n",
            "\n",
            "Shortness of breath\n",
            "\n",
            "SOB\n",
            "\n",
            "Somnolent\n",
            "\n",
            "Stop\n",
            "\n",
            "Subtherapeutic\n",
            "\n",
            "Suicide attempt\n",
            "\n",
            "Supratherapeutic\n",
            "\n",
            "SVT\n",
            "\n",
            "Switch\n",
            "\n",
            "Tamponade\n",
            "\n",
            "Temporal\n",
            "\n",
            "Thrombocytopenia\n",
            "\n",
            "Toxic\n",
            "\n",
            "Transfusion\n",
            "\n",
            "UGIB\n",
            "\n",
            "Unresponsive\n",
            "\n",
            "Vit K\n",
            "\n",
            "Vitamin K\n",
            "\n",
            "Withdrawal\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7mgDmeknRtT"
      },
      "source": [
        "def negation_count(x, row):\n",
        "    \"\"\"counts total no, not and n't in the sentence \"\"\" \n",
        "    sent = row['sentence_text']\n",
        "    count = 0\n",
        "    for word in nltk.word_tokenize(sent):\n",
        "        if word == 'not' or word == 'no' or word[-3:] == \"n't\":\n",
        "            count += 1\n",
        "    return count\n",
        "\n",
        "def negation_count2(x, row):\n",
        "    \"\"\" gives how many negative words in each segment\"\"\"\n",
        "    count = 0\n",
        "    drug1_i = row['drug1offset'].split('-')\n",
        "    drug2_i = row['drug2offset'].split('-')\n",
        "    sentence = row['sentence_text']\n",
        "    before_count=0\n",
        "    middle_count=0\n",
        "    end_count=0\n",
        "    first_phrase=\"\"\n",
        "    mid_phrase=\"\"\n",
        "    end_phrase=\"\"\n",
        "    try:\n",
        "      first_phrase = sentence[0:int(drug1_i[0])].split()\n",
        "      mid_phrase = sentence[int(float(drug1_i[-1]))+1:int(drug2_i[0])].split()\n",
        "      end_phrase = sentence[int(float(drug2_i[-1]))+1:].split()\n",
        "      before_count = len(set(first_phrase) & negative_words)\n",
        "      middle_count = len(set(mid_phrase) & negative_words)\n",
        "      end_count = len(set(end_phrase) & negative_words)\n",
        "\n",
        "    except ValueError:\n",
        "      print(\"ValueError\")\n",
        "    return before_count, middle_count, end_count\n",
        "    \n",
        "\n",
        "def trigger_count(x, row):\n",
        "    \"\"\" total number of trigger words \"\"\"\n",
        "    sent = row['sentence_text']\n",
        "    count = 0\n",
        "    for word in nltk.word_tokenize(sent.lower()):\n",
        "        if word in triggers:\n",
        "            count += 1\n",
        "    return count\n",
        "\n",
        "def trigger_count2(x, row):\n",
        "    ''' counts number of trigger words in each segment '''\n",
        "    count = 0\n",
        "    drug1_i = row['drug1offset'].split('-')\n",
        "    drug2_i = row['drug2offset'].split('-')\n",
        "    sentence = row['sentence_text']\n",
        "    first_phrase =\"\"\n",
        "    mid_phrase=\"\"\n",
        "    end_phrase=\"\"\n",
        "    try:\n",
        "      first_phrase = sentence[0:int(drug1_i[0])].split()\n",
        "      mid_phrase = sentence[int(float(drug1_i[-1]))+1:int(drug2_i[0])].split()\n",
        "      end_phrase = sentence[int(float(drug2_i[-1]))+1:].split()\n",
        "    except ValueError:\n",
        "      print(\"\")\n",
        "      #print(\"Value error in trigger_count2\")\n",
        "      #print(\"inside exception In trigger_count 2 first coubt, mid,end:\",first_phrase,mid_phrase,end_phrase)\n",
        "    before_count = len(set(first_phrase))# & trigger_words)\n",
        "    middle_count = len(set(mid_phrase))# & trigger_words)\n",
        "    end_count = len(set(end_phrase))# & trigger_words)\n",
        "    #print(\"with and wthout excep: sentance:\",sentence)\n",
        "    #print(\"In trigger_count 2 first coubt, mid,end:\",first_phrase,mid_phrase,end_phrase)\n",
        "    #print(\"passing befor_count,middle_count,end_count\",before_count,middle_count,end_count)\n",
        "    return before_count, middle_count, end_count\n",
        "\n",
        "def ultra_trigger(x, row):\n",
        "    ''' counts ultra-trigger words '''\n",
        "    sent = row['sentence_text']\n",
        "    words = ['coadministration', 'concomitant', 'concomitantly']\n",
        "    count = 0\n",
        "    for word in words:\n",
        "        if word in nltk.word_tokenize(sent.lower()):\n",
        "            count += 1\n",
        "    return count\n",
        "        \n",
        "def words_sep_by_and(x, row):\n",
        "    ''' binary feature if drugs are seperated only by and, not used '''\n",
        "    sent = row['sentence_text']\n",
        "    d1 = row['drug1']\n",
        "    d2 = row['drug2']\n",
        "    sent = sent.replace(d1, 'drug1')\n",
        "    sent = sent.replace(d2, 'drug2')\n",
        "    words = nltk.word_tokenize(sent)\n",
        "    if abs(words.index('drug1') - words.index('drug2')) < 3:\n",
        "        if words[words.index('drug1') + words.index('drug2')/2] == 'and':\n",
        "            return 1\n",
        "    return 0\n",
        "\n",
        "def key_phrase(x, row):\n",
        "    ''' binary feature that checks for key phrases, listed below'''\n",
        "    sent = row['sentence_text']\n",
        "    d1 = row['drug1']\n",
        "    d2 = row['drug2']\n",
        "    sent = sent.replace(d1, 'drug1')\n",
        "    sent = sent.replace(d2, 'drug2')\n",
        "    phrases = ['concurrent administration of drug1 and drug2', 'drug1 concurrently with drug2',\n",
        "            'co-administration of drug1 and drug2', 'coadministration of drug1 and drug2', \n",
        "               'concurrent use of drug1 and drug2',]\n",
        "    for phrase in phrases:\n",
        "        if phrase in sent.lower():\n",
        "            return 1\n",
        "    return 0\n",
        "    \n",
        "def num_words_between_drugs(x,row):\n",
        "    ''' counts number of words between drug mentions '''\n",
        "    drug1_i = row['drug1offset'].split('-')\n",
        "    drug2_i = row['drug2offset'].split('-')\n",
        "    sentence = row['sentence_text']\n",
        "    pre_phrase=\"\"\n",
        "    try:\n",
        "      pre_phrase = sentence[int(float(drug1_i[-1]))+1:int(float(drug2_i[0]))].split()\n",
        "    except ValueError:\n",
        "      print(\"eroor exception\")\n",
        "    return len(pre_phrase)\n",
        "\n",
        "def num_drugs_between_drugs(x, row):\n",
        "    ''' counts number of drugs between drug mentions '''\n",
        "    drug1_i = row['drug1offset'].split('-')\n",
        "    drug2_i = row['drug2offset'].split('-')\n",
        "    sentence = row['sentence_text']\n",
        "    pre_phrase=\"\"\n",
        "    try:\n",
        "\n",
        "      pre_phrase = sentence[int(float(drug1_i[-1]))+1:int(float(drug2_i[0]))]\n",
        "    except ValueError:\n",
        "      print(\"error\")\n",
        "    list_of_drugs = row['drugsinsent'].split(\",\")[0:-1]\n",
        "    drugs_in_bet = 0\n",
        "    for drug in list_of_drugs:\n",
        "        drug_split = drug.split()\n",
        "        if(pre_phrase.find(drug_split[0]) != -1):\n",
        "            drugs_in_bet += 1\n",
        "\n",
        "    return drugs_in_bet\n",
        "\n",
        "def verb_count(x, row):\n",
        "    '''counts number of verbs in each segment '''\n",
        "    pattern = r'''(?x)     \n",
        "    ([A-Z]\\.)+                       # abbrevations\n",
        "    |\\w+'\\w+                         # contractions\n",
        "    | \\w+(?:(-|(\\.\\s))(\\n)?\\w+)*     # words w/ internal hyphens, extend to next line, and with periods like Mrs. Reed\n",
        "    '''\n",
        "    count = 0\n",
        "    drug1_i = row['drug1offset'].split('-')\n",
        "    drug2_i = row['drug2offset'].split('-')\n",
        "    sentence = row['sentence_text']\n",
        "    first_phrase=\"\"\n",
        "    mid_phrase=\"\"\n",
        "    end_phrase=\"\"\n",
        "    try:\n",
        "      first_phrase= nltk.pos_tag(nltk.word_tokenize(sentence[0:int(drug1_i[0])]))\n",
        "      mid_phrase = nltk.pos_tag(nltk.word_tokenize(sentence[int(drug1_i[-1])+1:int(drug2_i[0])]))\n",
        "      end_phrase = nltk.pos_tag(nltk.word_tokenize(sentence[int(drug2_i[-1])+1:]))\n",
        "    except ValueError:\n",
        "      print(\"error\")\n",
        "    before_count, middle_count, end_count = 0,0,0\n",
        "    for tag in first_phrase:\n",
        "        if tag[1].startswith('V'):\n",
        "            before_count += 1\n",
        "    for tag in mid_phrase:\n",
        "        if tag[1].startswith('V'):\n",
        "            middle_count += 1\n",
        "    for tag in end_phrase:\n",
        "        if tag[1].startswith('V'):\n",
        "            end_count += 1\n",
        "    return before_count, middle_count, end_count\n",
        "\n",
        "def dist(idx, enum_list):\n",
        "    '''helper function for trig_dist '''\n",
        "    dif = [abs(idx-num) for num,word in enum_list]\n",
        "    if len(dif) == 0:\n",
        "        return 0\n",
        "    else:\n",
        "        return min(dif)\n",
        "    \n",
        "def trig_dist(x, row):\n",
        "    '''counts distance from both drug mentions to closest trigger'''\n",
        "    sentence = row['sentence_text']\n",
        "    sentence = sentence.replace(row['drug1'], 'drug1')\n",
        "    sentence = sentence.replace(row['drug2'], 'drug2')\n",
        "    words = nltk.word_tokenize(sentence)\n",
        "    \n",
        "    try:\n",
        "        drug1_i = words.index('drug1')\n",
        "    except:\n",
        "        drug1_i = 0\n",
        "    try:\n",
        "        drug2_i = words.index('drug2')\n",
        "    except:\n",
        "        drug2_i = 0\n",
        "    enum_words = list(enumerate(words))\n",
        "    enum_trigs = [(num,word) for num, word in enum_words if word in trigger_words]\n",
        "    \n",
        "    return dist(drug1_i, enum_trigs), dist(drug2_i, enum_trigs)\n",
        "\n",
        "def subj_obj(x, row):\n",
        "    '''binary feature that checks if drug1 is the nsubj or in its subtree \n",
        "    and if drug2 is dobj or in its subtree, returns 1 if both, 0 otherwise'''\n",
        "    d1, d2 = row['drug1'],row['drug2']\n",
        "    sent = row['sentence_text'].replace(d1,'drug1').replace(d2,'drug2')\n",
        "    doc = nlp(sent)\n",
        "    roots = [r for r in doc if r.head is r and r.pos == VERB] #and r.text in trigger_words]\n",
        "    d1 = [d for d in doc if d.text == 'drug1']\n",
        "    d2 = [d for d in doc if d.text == 'drug2']\n",
        "\n",
        "    flag1 = False\n",
        "    flag2 = False\n",
        "    for r in roots:\n",
        "        subj = [s for s in r.children if s.dep_[:5] == 'nsubj']\n",
        "        obj = [o for o in r.children if o.dep_[:4] == 'dobj']\n",
        "        for s in subj:\n",
        "            if 'drug1' == s.text:\n",
        "                flag1 = True\n",
        "            for s1 in s.subtree:\n",
        "                if s1.text == 'drug1':\n",
        "                    flag1 = True\n",
        "        for o in obj:\n",
        "            if 'drug2' == o.text:\n",
        "                flag2 = True\n",
        "            for o1 in o.subtree:\n",
        "                if o1.text == 'drug2':\n",
        "                    flag2 = True\n",
        "    if flag1 and flag2:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "    \n",
        "def trig_in_path(x, row):\n",
        "    ''' binary feature if theres a trigger word in dep path to root '''\n",
        "    d1, d2 = row['drug1'],row['drug2']\n",
        "    \n",
        "    sent = row['sentence_text'].replace(d1,'drug1').replace(d2,'drug2')\n",
        "    doc = nlp(sent)\n",
        "    #print(\"doc::::\",doc)\n",
        "    drugs = [d for d in doc if d.text == 'drug1' or d.text == 'drug2']\n",
        "    #print(\"Printin drus : \",drugs)\n",
        "    for d in drugs:\n",
        "        #print(\"for loop for trig_path\")\n",
        "        #print(\"d befoe:\",d)\n",
        "        d = d.head\n",
        "        #print(\"Printin drus d value after d.head: \",d)\n",
        "        \n",
        "        while d.head is not d:\n",
        "            #print(\"entered while\")\n",
        "            if d.text in trigger_words:\n",
        "               print(\"return 1\")\n",
        "               return 1\n",
        "            else:\n",
        "                #print(\"change -- d:\",d)\n",
        "                d = d.head\n",
        "                #print(\"change -- d after:\",d)\n",
        "            if d.head==d :\n",
        "                return 0\n",
        "    return 0\n",
        "    \n",
        "def both_head(x, row):\n",
        "    ''' checks if the head of both drug mentions is the same '''\n",
        "    d1, d2 = row['drug1'],row['drug2']\n",
        "    sent = row['sentence_text'].replace(d1,'drug1').replace(d2,'drug2')\n",
        "    doc = nlp(sent)\n",
        "    #roots = [r for r in doc if r.head is r and r.pos == VERB and r.text in trigger_words]\n",
        "    drugs = [d for d in doc if d.text == 'drug1' or d.text == 'drug2']\n",
        "    if  len(drugs) < 2:\n",
        "        return 0\n",
        "    if drugs[0].head == drugs[1].head: #and drugs[0].head.pos ==VERB:\n",
        "        return 1\n",
        "    return 0\n",
        "\n",
        "def verb_vect(x, row):\n",
        "    '''counts number of verbs in each segment '''\n",
        "    \n",
        "    sentence = row['sentence_text']\n",
        "    words = nltk.word_tokenize(sentence)\n",
        "    pos_words = nltk.pos_tag(words)\n",
        "    \n",
        "    try:\n",
        "        drug1_i = words.index(row['drug1'])\n",
        "        drug2_i = words.index(row['drug2'])\n",
        "    except:\n",
        "        return 0.0, 0.0, 0.0\n",
        "   \n",
        "    bef_verb = 0.0\n",
        "    try:\n",
        "        for i in range(drug1_i,0, -1):\n",
        "            if pos_words[i][1].startswith('V') or pos_words[i][1].startswith('N'):\n",
        "                bef_verb = np.mean(ddi_embedding_model[pos_words[i][0]])\n",
        "                break\n",
        "    except:\n",
        "        bef_verb = 0.0\n",
        "            \n",
        "    bet_verb = 0.0\n",
        "    try:\n",
        "        for i in range(drug2_i,drug1_i, -1):\n",
        "            if pos_words[i][1].startswith('V') or pos_words[i][1].startswith('N'):\n",
        "                bet_verb = np.mean(ddi_embedding_model[pos_words[i][0]])\n",
        "                break\n",
        "    except:\n",
        "        bet_verb = 0.0\n",
        "            \n",
        "            \n",
        "    aft_verb = 0.0   \n",
        "    try:\n",
        "        for i in range(drug2_i, len(pos_words)):\n",
        "            if pos_words[i][1].startswith('V') or pos_words[i][1].startswith('N'):\n",
        "                aft_verb = np.mean(ddi_embedding_model[pos_words[i][0]])\n",
        "                break\n",
        "    except:\n",
        "        aft_verb = 0.0\n",
        "\n",
        "    return bef_verb, bet_verb, aft_verb\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zp4cgMZKnqfO"
      },
      "source": [
        "def build_features(df):\n",
        "    c=0\n",
        "    \n",
        "\n",
        "    list_of_features = []\n",
        "    negation_count_l = []\n",
        "    trigger_count_l = []\n",
        "    ultra_trigger_l = []\n",
        "    subj_obj_l = []\n",
        "    both_head_l = []\n",
        "    trig_in_path_l = []\n",
        "    for index, row in df.iterrows():\n",
        "        negation_count_l.append(negation_count(index,row))\n",
        "        trigger_count_l.append(trigger_count(index,row))\n",
        "        ultra_trigger_l.append(ultra_trigger(index,row))\n",
        "        subj_obj_l.append(subj_obj(index,row))\n",
        "        both_head_l.append(both_head(index,row))\n",
        "        trig_in_path_l.append(trig_in_path(index,row))\n",
        "        \n",
        "    list_of_features.append(negation_count_l)\n",
        "    list_of_features.append(trigger_count_l)\n",
        "    list_of_features.append(ultra_trigger_l)\n",
        "    list_of_features.append(subj_obj_l)\n",
        "    list_of_features.append(both_head_l)\n",
        "    list_of_features.append(trig_in_path_l)\n",
        "\n",
        "    drugs_between_drugs = []\n",
        "    words_between_drugs = []\n",
        "    for index, row in df.iterrows():\n",
        "        drugs_between_drugs.append(num_drugs_between_drugs(index,row))\n",
        "        words_between_drugs.append(num_words_between_drugs(index,row))\n",
        "    list_of_features.append(drugs_between_drugs)\n",
        "    list_of_features.append(words_between_drugs)\n",
        "    \n",
        "    #words_sep_by_and_l = []\n",
        "    #key_phrase_l = []\n",
        "    #for index, row in df.iterrows():\n",
        "        #words_sep_by_and_l.append(words_sep_by_and(index,row))\n",
        "        #key_phrase_l.append(key_phrase(index,row))\n",
        "    #list_of_features.append(words_sep_by_and_l)\n",
        "    #list_of_features.append(key_phrase_l)\n",
        "    \n",
        "    btg, betg, atg = [], [], []\n",
        "    for index, row in df.iterrows():\n",
        "        before, between, after = trigger_count2(index,row)\n",
        "        btg.append(before)\n",
        "        betg.append(between)\n",
        "        atg.append(after)\n",
        "    list_of_features.append(btg)\n",
        "    list_of_features.append(betg)\n",
        "    list_of_features.append(atg)\n",
        "   \n",
        "    n_btg, n_betg, n_atg = [], [], []\n",
        "    for index, row in df.iterrows():\n",
        "        before, between, after = negation_count2(index,row)\n",
        "        n_btg.append(before)\n",
        "        n_betg.append(between)\n",
        "        n_atg.append(after)\n",
        "    list_of_features.append(n_btg)\n",
        "    list_of_features.append(n_betg)\n",
        "    list_of_features.append(n_atg)\n",
        "    \n",
        "    v_btg, v_betg, v_atg = [], [], []\n",
        "    for index, row in df.iterrows():\n",
        "        before, between, after = verb_count(index,row)\n",
        "        v_btg.append(before)\n",
        "        v_betg.append(between)\n",
        "        v_atg.append(after)\n",
        "    list_of_features.append(v_btg)\n",
        "    list_of_features.append(v_betg)\n",
        "    list_of_features.append(v_atg)\n",
        "    \n",
        "    word2vec_b, word2vec_bet, word2vec_a = [], [], []\n",
        "    for index, row in df.iterrows():\n",
        "        verb_bef, verb_bet, verb_aft = verb_vect(index, row)\n",
        "        word2vec_b.append(verb_bef)\n",
        "        word2vec_bet.append(verb_bet)\n",
        "        word2vec_a.append(verb_aft)\n",
        "        \n",
        "    list_of_features.append(word2vec_b)\n",
        "    list_of_features.append(word2vec_bet)\n",
        "    list_of_features.append(word2vec_a)\n",
        "\n",
        "    print(\"List_of_features\",list_of_features)\n",
        "    return list_of_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cof29uS9n2c6"
      },
      "source": [
        "vec = TfidfVectorizer(ngram_range=(1, 2), max_df=0.8, max_features=1000)\n",
        "arr_train_feature_sparse = vec.fit_transform(df_train['sentence_text'])\n",
        "\n",
        "arr_train_feature = arr_train_feature_sparse.toarray()\n",
        "\n",
        "\n",
        "arr_test_feature_sparse = vec.transform(df_test[\"sentence_text\"])\n",
        "arr_test_feature = arr_test_feature_sparse.toarray()\n",
        "\n",
        "arr_test_feature_sparse1 = vec.transform(idata[\"sentence_text\"])\n",
        "arr_test_feature1 = arr_test_feature_sparse1.toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6gKAQgprsAG"
      },
      "source": [
        "def convert_to_numpy(list_):\n",
        "    x = np.asarray(list_)\n",
        "    y = x.reshape(-1, 1)\n",
        "    return y\n",
        "\n",
        "def create_feature_set(mainset,features):\n",
        "    count=0\n",
        "    for list_f in features:\n",
        "        count=count+1\n",
        "        print(\"Feature loading:\")\n",
        "        print(count)\n",
        "        mainset = np.append(mainset, convert_to_numpy(list_f), axis=1)\n",
        "    return mainset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwN-HjEaJ2yQ",
        "outputId": "667c8b84-8b8a-4579-d444-c56c52b6be4a"
      },
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 484
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THpTA0u1rwzj",
        "outputId": "fb930924-4561-45fb-a008-2444a74e5fad"
      },
      "source": [
        "train_feature_list = build_features(df_train)\n",
        "print(\"arr_train_feature:\",arr_train_feature)\n",
        "#print(\"train_feature_list:\",train_feature_list)\n",
        "train_feature_set = create_feature_set(arr_train_feature, train_feature_list)\n",
        "#print(\"Train_feature_set:\",train_feature_set)\n",
        "test_feature_list = build_features(df_test)\n",
        "test_feature_set = create_feature_set(arr_test_feature, test_feature_list)\n",
        "\n",
        "test_feature_list1 = build_features(idata)\n",
        "test_feature_set1 = create_feature_set(arr_test_feature1, test_feature_list1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:289: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:298: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:308: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "List_of_features [[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 2, 2, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 2, 0, 1, 0, 0, 2, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 3, 0, 0, 2, 3, 6, 3, 0, 3, 0, 0, 1, 1, 3, 0, 1, 0, 5, 0, 0, 2, 1, 0, 5, 4, 5, 3, 4, 3, 2, 1, 2, 2, 0, 4, 0, 5, 0, 8, 0, 0, 1, 2, 0, 0, 0, 0, 0, 4], [2, 7, 2, 1, 4, 6, 8, 10, 2, 14, 1, 1, 2, 3, 6, 1, 3, 7, 8, 1, 2, 11, 2, 1, 11, 7, 6, 6, 15, 7, 4, 2, 3, 7, 1, 7, 1, 8, 1, 11, 1, 14, 2, 2, 1, 1, 3, 1, 2, 19], [19, 13, 20, 19, 14, 19, 21, 5, 12, 5, 4, 24, 5, 6, 12, 6, 13, 4, 20, 12, 0, 6, 17, 27, 12, 8, 25, 8, 5, 15, 10, 18, 7, 5, 15, 7, 15, 8, 7, 18, 20, 4, 14, 26, 4, 3, 11, 2, 2, 0], [2, 7, 2, 1, 4, 6, 8, 10, 2, 14, 1, 1, 2, 3, 6, 1, 3, 7, 8, 1, 2, 11, 2, 1, 11, 7, 6, 6, 15, 7, 4, 2, 3, 7, 1, 7, 1, 8, 1, 11, 1, 11, 2, 2, 1, 1, 3, 1, 2, 17], [9, 16, 12, 12, 6, 7, 1, 1, 10, 4, 10, 7, 10, 5, 6, 10, 19, 13, 4, 11, 13, 4, 13, 4, 1, 9, 1, 10, 3, 12, 9, 12, 13, 7, 19, 10, 7, 7, 16, 3, 9, 1, 7, 4, 21, 3, 8, 12, 15, 7], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1], [1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 0, 2, 1, 1, 1, 1, 1, 0, 1, 3, 0, 2, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 0, 1, 2, 0, 0, 2, 0, 1, 0], [0, 1, 0, 0, 0, 1, 0, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 2], [0, 2, 2, 1, 0, 0, 0, 0, 0, 0, 2, 0, 3, 2, 0, 3, 3, 0, 0, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 2, 2, 1, 3, 2], [0.0, -0.00020461298, 0.0, -0.00019146853, 5.533691e-05, -0.00019146853, 0.0, -0.00044110202, 0.00026390975, 0.0, 0.0, -0.0005488581, -0.00044110202, 0.0, 0.00026390975, -0.00019146853, -0.00020461298, 0.0, -0.00043577806, 0.0, 0.0, 0.0, -0.000109228065, -0.00043787487, 0.00026390975, -0.00043577806, -0.00017964287, -0.00043577806, 0.0, 0.0, 0.0, -0.00044110202, 0.0, 0.0, -0.00014282914, -0.0003602333, 0.0, 0.0, -0.0003602333, -0.00044110202, 0.0, -0.00020461298, 0.0, -0.00026652013, 0.0, 0.0, -0.000109228065, 0.0, -0.00020461298, 0.0], [0.0, 0.00013542175, 0.0, -0.00043577806, -0.00048336762, -0.00017964287, 0.0, 0.0, 5.533691e-05, 0.0, -0.00020461298, -0.00017964287, -0.00043577806, 0.0, -0.00048336762, -0.00043577806, -0.00027796213, 0.0, -0.00043787487, 0.0, 0.0, 0.0, -0.00019146853, 0.0, 6.353998e-05, -0.00020092087, -3.971914e-05, 5.533691e-05, 0.0, 0.0, 0.0, -0.00043577806, 0.0, 0.0, -0.00027796213, 5.533691e-05, 0.0, 0.0, -0.00043577806, -0.00039689254, 0.0, 0.0, 0.0, -0.00043787487, 0.0, 0.0, -3.971914e-05, -0.00020461298, 0.0, 0.0], [0.0, 0.00013542175, 0.0, -0.00043577806, -0.00048336762, -0.00017964287, 0.0, 0.0, 5.533691e-05, 0.0, -0.00020461298, -0.00017964287, -0.00043577806, 0.0, -0.00048336762, -0.00043577806, -0.00027796213, 0.0, -0.00039689254, 0.0, 0.0, 0.0, -0.00019146853, -0.00039689254, 6.353998e-05, -0.00020092087, -3.971914e-05, 5.533691e-05, 0.0, 0.0, 0.0, -0.00043577806, 0.0, 0.0, -0.00027796213, 5.533691e-05, 0.0, 0.0, -0.00043577806, -0.00039689254, 0.0, 0.0, 0.0, -0.00039689254, 0.0, 0.0, -3.971914e-05, -0.00020461298, 0.0, 0.0]]\n",
            "arr_train_feature: [[0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.12187634 0.         0.        ]\n",
            " [0.         0.         0.         ... 0.12187634 0.         0.        ]\n",
            " ...\n",
            " [0.17194462 0.17194462 0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]]\n",
            "Feature loading:\n",
            "1\n",
            "Feature loading:\n",
            "2\n",
            "Feature loading:\n",
            "3\n",
            "Feature loading:\n",
            "4\n",
            "Feature loading:\n",
            "5\n",
            "Feature loading:\n",
            "6\n",
            "Feature loading:\n",
            "7\n",
            "Feature loading:\n",
            "8\n",
            "Feature loading:\n",
            "9\n",
            "Feature loading:\n",
            "10\n",
            "Feature loading:\n",
            "11\n",
            "Feature loading:\n",
            "12\n",
            "Feature loading:\n",
            "13\n",
            "Feature loading:\n",
            "14\n",
            "Feature loading:\n",
            "15\n",
            "Feature loading:\n",
            "16\n",
            "Feature loading:\n",
            "17\n",
            "Feature loading:\n",
            "18\n",
            "Feature loading:\n",
            "19\n",
            "Feature loading:\n",
            "20\n",
            "error\n",
            "eroor exception\n",
            "error\n",
            "eroor exception\n",
            "error\n",
            "eroor exception\n",
            "error\n",
            "eroor exception\n",
            "error\n",
            "eroor exception\n",
            "error\n",
            "eroor exception\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ValueError\n",
            "ValueError\n",
            "ValueError\n",
            "ValueError\n",
            "ValueError\n",
            "ValueError\n",
            "ValueError\n",
            "error\n",
            "error\n",
            "error\n",
            "error\n",
            "error\n",
            "error\n",
            "error\n",
            "List_of_features [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 2, 4, 4, 0, 4, 0, 0, 0, 1, 0, 0, 0, 2, 3, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 0, 1, 2, 3, 0, 1, 2, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 14, 15, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 13, 14, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 12, 13, 0, 1, 2, 3, 4, 5, 6, 7, 8, 11, 12, 0, 1, 2, 3, 4, 5, 6, 7, 10, 11, 0, 1, 2, 3, 4, 5, 6, 9, 10, 0, 1, 2, 3, 4, 5, 8, 9, 0, 1, 2, 3, 4, 7, 8, 0, 1, 2, 3, 6, 7, 0, 1, 2, 5, 6, 0, 1, 4, 5, 0, 3, 4, 0, 1, 0, 0, 0, 1, 2, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1], [8, 10, 15, 17, 1, 8, 4, 1, 1, 9, 7, 8, 4, 15, 26, 10, 10, 12, 8, 2, 10, 7, 1, 6, 9, 2, 1, 6, 10, 3, 6, 10, 3, 9, 13, 14, 15, 16, 18, 3, 4, 5, 6, 8, 1, 2, 3, 5, 1, 2, 4, 1, 3, 2, 3, 16, 12, 1, 4, 8, 1, 10, 10, 0, 0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 26, 29, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 13, 24, 27, 1, 2, 3, 4, 5, 6, 7, 8, 9, 12, 23, 26, 1, 2, 3, 4, 5, 6, 7, 8, 11, 22, 25, 1, 2, 3, 4, 5, 6, 7, 10, 21, 24, 1, 2, 3, 4, 5, 6, 9, 20, 23, 1, 2, 3, 4, 5, 8, 19, 22, 1, 2, 3, 4, 7, 18, 21, 1, 2, 3, 6, 17, 20, 1, 2, 5, 16, 19, 1, 4, 15, 18, 2, 13, 16, 11, 14, 3, 1, 4, 5, 7, 1, 3, 2, 0, 0, 13, 10, 12, 2, 23, 26, 2, 1, 2, 7, 0, 4, 1, 1, 5, 3, 9, 1, 8, 0, 5, 1, 7, 5, 4, 1, 19, 24, 3, 2, 18], [3, 3, 3, 3, 11, 11, 13, 15, 5, 5, 7, 0, 0, 0, 0, 5, 15, 8, 17, 0, 0, 2, 7, 0, 0, 6, 2, 0, 0, 6, 0, 0, 6, 0, 0, 0, 0, 0, 0, 11, 11, 11, 11, 11, 15, 15, 15, 15, 16, 16, 16, 17, 17, 18, 0, 0, 3, 4, 4, 4, 9, 9, 13, 2, 2, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 19, 19, 19, 19, 20, 20, 20, 22, 22, 33, 11, 0, 0, 0, 5, 5, 6, 2, 2, 9, 0, 0, 11, 6, 6, 27, 8, 0, 0, 2, 3, 8, 0, 0, 1, 3, 13, 0, 2, 13, 0, 0, 1, 4, 9, 0, 0, 16, 10, 10], [8, 10, 12, 13, 1, 7, 4, 1, 1, 9, 7, 8, 4, 14, 23, 10, 10, 10, 8, 2, 10, 7, 1, 6, 9, 2, 1, 6, 10, 3, 6, 10, 3, 9, 13, 14, 15, 16, 18, 3, 4, 5, 6, 8, 1, 2, 3, 5, 1, 2, 4, 1, 3, 2, 3, 16, 12, 1, 4, 8, 1, 9, 9, 0, 0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 26, 29, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 13, 24, 27, 1, 2, 3, 4, 5, 6, 7, 8, 9, 12, 23, 26, 1, 2, 3, 4, 5, 6, 7, 8, 11, 22, 25, 1, 2, 3, 4, 5, 6, 7, 10, 21, 24, 1, 2, 3, 4, 5, 6, 9, 20, 23, 1, 2, 3, 4, 5, 8, 19, 22, 1, 2, 3, 4, 7, 18, 21, 1, 2, 3, 6, 17, 20, 1, 2, 5, 16, 19, 1, 4, 15, 18, 2, 13, 16, 11, 14, 3, 1, 4, 5, 7, 1, 3, 2, 0, 0, 12, 10, 12, 2, 21, 23, 2, 1, 2, 7, 0, 4, 1, 1, 5, 3, 9, 1, 8, 0, 5, 1, 7, 5, 4, 1, 16, 20, 3, 2, 17], [8, 7, 2, 1, 7, 1, 2, 1, 8, 1, 1, 1, 21, 11, 1, 11, 1, 17, 8, 10, 2, 2, 6, 7, 4, 4, 8, 7, 3, 3, 7, 3, 3, 9, 6, 5, 4, 3, 1, 6, 5, 4, 3, 1, 5, 4, 3, 1, 4, 3, 1, 3, 1, 1, 13, 1, 1, 4, 12, 12, 12, 3, 3, 0, 0, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 18, 5, 1, 28, 27, 26, 25, 24, 23, 22, 21, 20, 18, 16, 5, 1, 27, 26, 25, 24, 23, 22, 21, 20, 18, 16, 5, 1, 26, 25, 24, 23, 22, 21, 20, 18, 16, 5, 1, 25, 24, 23, 22, 21, 20, 18, 16, 5, 1, 24, 23, 22, 21, 20, 18, 16, 5, 1, 23, 22, 21, 20, 18, 16, 5, 1, 22, 21, 20, 18, 16, 5, 1, 21, 20, 18, 16, 5, 1, 20, 18, 16, 5, 1, 18, 16, 5, 1, 16, 5, 1, 5, 1, 1, 1, 11, 10, 8, 10, 8, 8, 0, 0, 5, 4, 1, 1, 3, 1, 1, 2, 0, 1, 0, 4, 2, 13, 10, 10, 2, 1, 1, 0, 1, 6, 1, 1, 4, 2, 9, 6, 6, 17, 3], [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 1, 0, 0, 0, 0, 1, 2, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 3, 0, 0, 0, 1, 1, 1, 0, 0, 2, 0, 0, 3, 0, 0, 6, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 3, 0, 0, 4, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0], [2, 2, 2, 2, 0, 0, 0, 0, 0, 3, 3, 1, 1, 2, 3, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 2, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 2, 2, 0, 0, 0, 2, 2, 0, 0, 2, 2, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 6, 6, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 3, 0, 2, 0, 2, 0, 1, 1, 1, 0, 1, 1, 0, 1, 3], [0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 2, 1, 0, 1, 0, 2, 1, 1, 0, 0, 2, 4, 3, 3, 4, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 0, 0, 2, 2, 2, 0, 0, 2, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 2, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 2, 3, 3, 3, 3, 1], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.00020092087, -0.00020092087, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.00020767543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.00020092087, -0.00020092087, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "Feature loading:\n",
            "1\n",
            "Feature loading:\n",
            "2\n",
            "Feature loading:\n",
            "3\n",
            "Feature loading:\n",
            "4\n",
            "Feature loading:\n",
            "5\n",
            "Feature loading:\n",
            "6\n",
            "Feature loading:\n",
            "7\n",
            "Feature loading:\n",
            "8\n",
            "Feature loading:\n",
            "9\n",
            "Feature loading:\n",
            "10\n",
            "Feature loading:\n",
            "11\n",
            "Feature loading:\n",
            "12\n",
            "Feature loading:\n",
            "13\n",
            "Feature loading:\n",
            "14\n",
            "Feature loading:\n",
            "15\n",
            "Feature loading:\n",
            "16\n",
            "Feature loading:\n",
            "17\n",
            "Feature loading:\n",
            "18\n",
            "Feature loading:\n",
            "19\n",
            "Feature loading:\n",
            "20\n",
            "List_of_features [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 1], [2, 10], [2, 2], [2, 10], [7, 1], [0, 0], [0, 0], [0, 0], [1, 1], [0, 1], [1, 0], [0.0, -8.833982e-06], [0.0, 0.0], [0.0, 0.0]]\n",
            "Feature loading:\n",
            "1\n",
            "Feature loading:\n",
            "2\n",
            "Feature loading:\n",
            "3\n",
            "Feature loading:\n",
            "4\n",
            "Feature loading:\n",
            "5\n",
            "Feature loading:\n",
            "6\n",
            "Feature loading:\n",
            "7\n",
            "Feature loading:\n",
            "8\n",
            "Feature loading:\n",
            "9\n",
            "Feature loading:\n",
            "10\n",
            "Feature loading:\n",
            "11\n",
            "Feature loading:\n",
            "12\n",
            "Feature loading:\n",
            "13\n",
            "Feature loading:\n",
            "14\n",
            "Feature loading:\n",
            "15\n",
            "Feature loading:\n",
            "16\n",
            "Feature loading:\n",
            "17\n",
            "Feature loading:\n",
            "18\n",
            "Feature loading:\n",
            "19\n",
            "Feature loading:\n",
            "20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAf9PPiicMTP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24e0f975-f235-41e9-f7e8-45a7dbbb3c3a"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import f1_score\n",
        "mn = MultinomialNB()\n",
        "print(\"Baseline with only vectorized bag of words:\")\n",
        "\n",
        "print(\"starting to fit\")\n",
        "mn_model = mn.fit(arr_train_feature, df_train.ddi_label)\n",
        "\n",
        "print(\"starting to predict\")\n",
        "pred_test = mn_model.predict(arr_test_feature)\n",
        "#print(\"test_labels:prediction:\",str(df_test.ddi_label),\":\",pred_test)\n",
        "pred_test1 = mn_model.predict(arr_test_feature1)\n",
        "print(\"input_pred:\",pred_test1)\n",
        "\n",
        "print('accuracy: ', accuracy_score(df_test.ddi_label, pred_test))\n",
        "print(f1_score(df_test.ddi_label, pred_test, average=None, pos_label = 1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Baseline with only vectorized bag of words:\n",
            "starting to fit\n",
            "starting to predict\n",
            "input_pred: [0 0]\n",
            "accuracy:  0.8755980861244019\n",
            "[0.93367347 0.        ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnXlAVi5cdxB"
      },
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import svm\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "def run_model(model):\n",
        "    print(\"starting to fit\")\n",
        "    fit_model = model.fit(train_feature_set, df_train.ddi_label)\n",
        "    print(\"starting to predict\")\n",
        "    pred_test = fit_model.predict(test_feature_set)\n",
        "    #input\n",
        "    pred_test1 = fit_model.predict(test_feature_set1)\n",
        "    print(\"input_pred:\",pred_test1)\n",
        "\n",
        "    print(\"df_test.ddi_label:\",df_test.ddi_label)\n",
        "    print(\"pred:\",pred_test)\n",
        "    print('accuracy: ', accuracy_score(df_test.ddi_label, pred_test))\n",
        "    print(f1_score(df_test.ddi_label, pred_test, average=None, pos_label = 1))\n",
        "    \n",
        "def gridsearch(model, parameters):\n",
        "    clf = GridSearchCV(model, parameters)\n",
        "    clf.fit(train_feature_set, df_train.ddi_label)\n",
        "    clf_pred_test = clf.predict(test_feature_set)\n",
        "    #inp\n",
        "    clf_p=clf.predict(test_feature_set1) \n",
        "   \n",
        "    print(\"inp:\",clf_p)\n",
        "    print('accuracy: ', accuracy_score(df_test.ddi_label, clf_pred_test))\n",
        "    print(f1_score(df_test.ddi_label, clf_pred_test, average=None, pos_label = 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSaycq2XcnV0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4b0fab7-c92c-48a5-9fec-a93eabd96d19"
      },
      "source": [
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "run_model(knn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "starting to fit\n",
            "starting to predict\n",
            "input_pred: [0 0]\n",
            "df_test.ddi_label: 0      0\n",
            "1      0\n",
            "2      0\n",
            "3      0\n",
            "4      0\n",
            "      ..\n",
            "219    0\n",
            "220    0\n",
            "221    1\n",
            "222    0\n",
            "223    0\n",
            "Name: ddi_label, Length: 209, dtype: int64\n",
            "pred: [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 1 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0]\n",
            "accuracy:  0.8851674641148325\n",
            "[0.9375     0.29411765]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQIzR25JimeF",
        "outputId": "12caaea3-13f7-4259-88ba-1ee7bfe2f54d"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "knn_cv = KNeighborsClassifier()\n",
        "param = {'n_neighbors': [3, 5, 8],\n",
        "             'weights': ['uniform', 'distance'],\n",
        "             'leaf_size': [30, 50, 75, 100]}\n",
        "gridsearch(knn_cv, param)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "inp: [0 0]\n",
            "accuracy:  0.8755980861244019\n",
            "[0.93367347 0.        ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNa_7BwrkMx8",
        "outputId": "c19d50ff-7066-4641-b382-baab9ecb7757"
      },
      "source": [
        "ada = AdaBoostClassifier(n_estimators=100)\n",
        "run_model(ada)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "starting to fit\n",
            "starting to predict\n",
            "input_pred: [1 1]\n",
            "df_test.ddi_label: 0      0\n",
            "1      0\n",
            "2      0\n",
            "3      0\n",
            "4      0\n",
            "      ..\n",
            "219    0\n",
            "220    0\n",
            "221    1\n",
            "222    0\n",
            "223    0\n",
            "Name: ddi_label, Length: 209, dtype: int64\n",
            "pred: [1 1 1 1 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1\n",
            " 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 1 1 0 1 1 0 0 0 0\n",
            " 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 0 0 0]\n",
            "accuracy:  0.7464114832535885\n",
            "[0.84457478 0.31168831]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0nQyTvbkRIg",
        "outputId": "006c036a-427f-4112-a27c-a60abd3589d8"
      },
      "source": [
        "ada_cv = AdaBoostClassifier()\n",
        "param = {'n_estimators': [50, 75, 100, 150, 200],\n",
        "        'learning_rate': [0.5, 1.0, 1.15]}\n",
        "gridsearch(ada_cv, param)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "inp: [1 1]\n",
            "accuracy:  0.7464114832535885\n",
            "[0.84548105 0.29333333]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "_Aq2vSu4kUet",
        "outputId": "1bb3d202-d49f-49f8-f5b4-82ba4a6cdad6"
      },
      "source": [
        "nb = MultinomialNB()\n",
        "run_model(nb)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "starting to fit\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-492-dd1380822e67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-487-b5be811d9929>\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"starting to fit\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mfit_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_feature_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mddi_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"starting to predict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mpred_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_feature_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_counters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_effective_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m         \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_feature_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_count\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    754\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m         \u001b[0;34m\"\"\"Count and smooth feature occurrences.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 756\u001b[0;31m         \u001b[0mcheck_non_negative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MultinomialNB (input X)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    757\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_count_\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_count_\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_non_negative\u001b[0;34m(X, whom)\u001b[0m\n\u001b[1;32m    992\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mX_min\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 994\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Negative values in data passed to %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mwhom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    995\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Negative values in data passed to MultinomialNB (input X)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qVE5qwqlkYJ4",
        "outputId": "7b7b4983-1523-4d58-b0a3-8363de841b1c"
      },
      "source": [
        "multi_cv = MultinomialNB()\n",
        "param = {'alpha': [0.8,0.9,1.0,1.1,1.2]}\n",
        "gridsearch(multi_cv, param)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Negative values in data passed to MultinomialNB (input X)\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Negative values in data passed to MultinomialNB (input X)\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Negative values in data passed to MultinomialNB (input X)\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Negative values in data passed to MultinomialNB (input X)\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Negative values in data passed to MultinomialNB (input X)\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Negative values in data passed to MultinomialNB (input X)\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Negative values in data passed to MultinomialNB (input X)\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Negative values in data passed to MultinomialNB (input X)\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Negative values in data passed to MultinomialNB (input X)\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Negative values in data passed to MultinomialNB (input X)\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Negative values in data passed to MultinomialNB (input X)\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Negative values in data passed to MultinomialNB (input X)\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Negative values in data passed to MultinomialNB (input X)\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Negative values in data passed to MultinomialNB (input X)\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Negative values in data passed to MultinomialNB (input X)\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Negative values in data passed to MultinomialNB (input X)\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Negative values in data passed to MultinomialNB (input X)\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Negative values in data passed to MultinomialNB (input X)\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Negative values in data passed to MultinomialNB (input X)\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Negative values in data passed to MultinomialNB (input X)\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Negative values in data passed to MultinomialNB (input X)\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Negative values in data passed to MultinomialNB (input X)\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Negative values in data passed to MultinomialNB (input X)\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Negative values in data passed to MultinomialNB (input X)\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Negative values in data passed to MultinomialNB (input X)\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-493-8e6f9e9768ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmulti_cv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'alpha'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1.2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mgridsearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmulti_cv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-487-b5be811d9929>\u001b[0m in \u001b[0;36mgridsearch\u001b[0;34m(model, parameters)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgridsearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_feature_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mddi_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mclf_pred_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_feature_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m#inp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    737\u001b[0m             \u001b[0mrefit_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_counters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_effective_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m         \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_feature_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_count\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    754\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m         \u001b[0;34m\"\"\"Count and smooth feature occurrences.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 756\u001b[0;31m         \u001b[0mcheck_non_negative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MultinomialNB (input X)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    757\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_count_\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_count_\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_non_negative\u001b[0;34m(X, whom)\u001b[0m\n\u001b[1;32m    992\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mX_min\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 994\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Negative values in data passed to %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mwhom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    995\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Negative values in data passed to MultinomialNB (input X)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZrSjnI-kbJs",
        "outputId": "beb9f256-2ea7-451c-e4d5-1f4f0f52dfd7"
      },
      "source": [
        "log_reg = LogisticRegression()\n",
        "run_model(log_reg)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "starting to fit\n",
            "starting to predict\n",
            "input_pred: [0 0]\n",
            "df_test.ddi_label: 0      0\n",
            "1      0\n",
            "2      0\n",
            "3      0\n",
            "4      0\n",
            "      ..\n",
            "219    0\n",
            "220    0\n",
            "221    1\n",
            "222    0\n",
            "223    0\n",
            "Name: ddi_label, Length: 209, dtype: int64\n",
            "pred: [0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0]\n",
            "accuracy:  0.8421052631578947\n",
            "[0.91338583 0.10810811]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09Kz6oC2kfPj",
        "outputId": "c28f2124-b189-4aa1-fdd4-8c32028a0b3a"
      },
      "source": [
        "log_cv = LogisticRegression()\n",
        "param = {'C': [0.5, 0.75, 1.0],\n",
        "        'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag']}\n",
        "gridsearch(log_cv, param)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "inp: [0 0]\n",
            "accuracy:  0.8516746411483254\n",
            "[0.91906005 0.11428571]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgrxVLVwkkrN",
        "outputId": "3cd79ada-d562-4736-d118-0d45b7e5013b"
      },
      "source": [
        "\n",
        "svm_model = svm.SVC()\n",
        "run_model(svm_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "starting to fit\n",
            "starting to predict\n",
            "input_pred: [0 0]\n",
            "df_test.ddi_label: 0      0\n",
            "1      0\n",
            "2      0\n",
            "3      0\n",
            "4      0\n",
            "      ..\n",
            "219    0\n",
            "220    0\n",
            "221    1\n",
            "222    0\n",
            "223    0\n",
            "Name: ddi_label, Length: 209, dtype: int64\n",
            "pred: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "accuracy:  0.8755980861244019\n",
            "[0.93367347 0.        ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFj0uTBQkln4",
        "outputId": "e87b1909-a0d2-4b1f-f052-ae3c7b2dde16"
      },
      "source": [
        "rfc = RandomForestClassifier()\n",
        "run_model(rfc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "starting to fit\n",
            "starting to predict\n",
            "input_pred: [0 0]\n",
            "df_test.ddi_label: 0      0\n",
            "1      0\n",
            "2      0\n",
            "3      0\n",
            "4      0\n",
            "      ..\n",
            "219    0\n",
            "220    0\n",
            "221    1\n",
            "222    0\n",
            "223    0\n",
            "Name: ddi_label, Length: 209, dtype: int64\n",
            "pred: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "accuracy:  0.8755980861244019\n",
            "[0.93367347 0.        ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NakjdjpkpSF",
        "outputId": "5acbcbda-b44d-4cbc-94e3-92a425ea9aee"
      },
      "source": [
        "rfc_cv = RandomForestClassifier()\n",
        "params = {'n_estimators': [5, 10, 15, 20],\n",
        "         'max_depth': [100, 200, 300]}\n",
        "gridsearch(rfc_cv, params)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "inp: [0 0]\n",
            "accuracy:  0.8803827751196173\n",
            "[0.93606138 0.07407407]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}